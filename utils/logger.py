"""
è®­ç»ƒæ—¥å¿—è®°å½•æ¨¡å—
è´Ÿè´£è®°å½•è®­ç»ƒè¿‡ç¨‹ä¸­çš„å„ç§æŒ‡æ ‡ï¼ŒåŒ…æ‹¬ï¼š
- å¥–åŠ±æ›²çº¿
- æŸå¤±å‡½æ•°å€¼
- è®­ç»ƒç»Ÿè®¡ä¿¡æ¯
- TensorBoardå¯è§†åŒ–
"""

import os
import json
import time
import numpy as np
from collections import deque, defaultdict
from datetime import datetime

try:
    from torch.utils.tensorboard import SummaryWriter
    TENSORBOARD_AVAILABLE = True
except ImportError:
    print("Warning: TensorBoard not available. Install with 'pip install tensorboard'")
    TENSORBOARD_AVAILABLE = False

from config import Config


class TrainingLogger:
    """
    è®­ç»ƒæ—¥å¿—è®°å½•å™¨
    
    åŠŸèƒ½ï¼š
    1. è®°å½•è®­ç»ƒè¿‡ç¨‹ä¸­çš„å„ç§æŒ‡æ ‡
    2. è®¡ç®—æ»‘åŠ¨å¹³å‡å€¼
    3. ä¿å­˜è®­ç»ƒæ—¥å¿—åˆ°æ–‡ä»¶
    4. TensorBoardå¯è§†åŒ–æ”¯æŒ
    5. æ€§èƒ½ç»Ÿè®¡
    """
    
    def __init__(self, log_dir=None, experiment_name=None):
        """
        åˆå§‹åŒ–æ—¥å¿—è®°å½•å™¨
        
        Args:
            log_dir (str): æ—¥å¿—ä¿å­˜ç›®å½•
            experiment_name (str): å®žéªŒåç§°
        """
        # è®¾ç½®æ—¥å¿—ç›®å½•
        if log_dir is None:
            log_dir = Config.LOG_DIR
        
        # åˆ›å»ºå®žéªŒç‰¹å®šçš„æ—¥å¿—ç›®å½•
        if experiment_name is None:
            experiment_name = f"mario_ppo_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        self.log_dir = os.path.join(log_dir, experiment_name)
        os.makedirs(self.log_dir, exist_ok=True)
        
        # åˆå§‹åŒ–TensorBoardå†™å…¥å™¨
        self.tensorboard_writer = None
        if Config.TENSORBOARD_LOG and TENSORBOARD_AVAILABLE:
            self.tensorboard_writer = SummaryWriter(self.log_dir)
        
        # è®­ç»ƒæŒ‡æ ‡å­˜å‚¨
        self.metrics = defaultdict(list)
        self.episode_metrics = defaultdict(list)
        
        # æ»‘åŠ¨å¹³å‡è®¡ç®—å™¨ï¼ˆç”¨äºŽå¹³æ»‘æ›²çº¿æ˜¾ç¤ºï¼‰
        self.running_averages = defaultdict(lambda: deque(maxlen=100))
        
        # è®­ç»ƒç»Ÿè®¡
        self.start_time = time.time()
        self.episode_count = 0
        self.step_count = 0
        self.update_count = 0
        
        # æœ€ä½³æ€§èƒ½è®°å½•
        self.best_reward = float('-inf')
        self.best_episode = 0
        
        print(f"æ—¥å¿—å°†ä¿å­˜åˆ°: {self.log_dir}")
        
    def log_episode(self, episode_reward, episode_length, info=None):
        """
        è®°å½•å›žåˆç»“æŸæ—¶çš„ä¿¡æ¯
        
        Args:
            episode_reward (float): å›žåˆæ€»å¥–åŠ±
            episode_length (int): å›žåˆé•¿åº¦ï¼ˆæ­¥æ•°ï¼‰
            info (dict): é¢å¤–çš„æ¸¸æˆä¿¡æ¯
        """
        self.episode_count += 1
        
        # è®°å½•åŸºæœ¬æŒ‡æ ‡
        self.episode_metrics['reward'].append(episode_reward)
        self.episode_metrics['length'].append(episode_length)
        
        # æ›´æ–°æ»‘åŠ¨å¹³å‡
        self.running_averages['reward'].append(episode_reward)
        self.running_averages['length'].append(episode_length)
        
        # è®°å½•æ¸¸æˆç‰¹å®šä¿¡æ¯
        if info:
            for key, value in info.items():
                if isinstance(value, (int, float)):
                    self.episode_metrics[key].append(value)
                    self.running_averages[key].append(value)
        
        # æ›´æ–°æœ€ä½³æ€§èƒ½
        if episode_reward > self.best_reward:
            self.best_reward = episode_reward
            self.best_episode = self.episode_count
        
        # TensorBoardè®°å½•
        if self.tensorboard_writer:
            self.tensorboard_writer.add_scalar('Episode/Reward', episode_reward, self.episode_count)
            self.tensorboard_writer.add_scalar('Episode/Length', episode_length, self.episode_count)
            self.tensorboard_writer.add_scalar('Episode/Reward_MA', np.mean(self.running_averages['reward']), self.episode_count)
            
            if info:
                for key, value in info.items():
                    if isinstance(value, (int, float)):
                        self.tensorboard_writer.add_scalar(f'Episode/{key}', value, self.episode_count)
    
    def log_training_step(self, **metrics):
        """
        è®°å½•è®­ç»ƒæ­¥éª¤ä¸­çš„æŒ‡æ ‡
        
        Args:
            **metrics: å„ç§è®­ç»ƒæŒ‡æ ‡ï¼ˆloss, learning_rateç­‰ï¼‰
        """
        self.step_count += 1
        
        for key, value in metrics.items():
            if isinstance(value, (int, float, np.floating, np.integer)):
                self.metrics[key].append(float(value))
                
                # TensorBoardè®°å½•
                if self.tensorboard_writer:
                    self.tensorboard_writer.add_scalar(f'Training/{key}', value, self.step_count)
    
    def log_update(self, **metrics):
        """
        è®°å½•PPOæ›´æ–°æ—¶çš„æŒ‡æ ‡
        
        Args:
            **metrics: PPOç›¸å…³æŒ‡æ ‡ï¼ˆpolicy_loss, value_loss, entropyç­‰ï¼‰
        """
        self.update_count += 1
        
        for key, value in metrics.items():
            if isinstance(value, (int, float, np.floating, np.integer)):
                self.metrics[f'update_{key}'].append(float(value))
                
                # TensorBoardè®°å½•
                if self.tensorboard_writer:
                    self.tensorboard_writer.add_scalar(f'Update/{key}', value, self.update_count)
    
    def log_system_info(self, **info):
        """
        è®°å½•ç³»ç»Ÿä¿¡æ¯ï¼ˆå†…å­˜ä½¿ç”¨ã€GPUåˆ©ç”¨çŽ‡ç­‰ï¼‰
        
        Args:
            **info: ç³»ç»Ÿä¿¡æ¯å­—å…¸
        """
        if self.tensorboard_writer:
            for key, value in info.items():
                if isinstance(value, (int, float, np.floating, np.integer)):
                    self.tensorboard_writer.add_scalar(f'System/{key}', value, self.step_count)
    
    def get_recent_average(self, metric_name, window=100):
        """
        èŽ·å–æŒ‡å®šæŒ‡æ ‡çš„è¿‘æœŸå¹³å‡å€¼
        
        Args:
            metric_name (str): æŒ‡æ ‡åç§°
            window (int): å¹³å‡çª—å£å¤§å°
            
        Returns:
            float: å¹³å‡å€¼ï¼Œå¦‚æžœæ•°æ®ä¸è¶³åˆ™è¿”å›žNone
        """
        if metric_name in self.running_averages and len(self.running_averages[metric_name]) > 0:
            return np.mean(list(self.running_averages[metric_name])[-window:])
        return None
    
    def print_training_stats(self):
        """
        æ‰“å°è®­ç»ƒç»Ÿè®¡ä¿¡æ¯
        """
        if self.episode_count == 0:
            return
        
        current_time = time.time()
        elapsed_time = current_time - self.start_time
        
        # è®¡ç®—å„ç§å¹³å‡å€¼
        avg_reward = self.get_recent_average('reward', 100)
        avg_length = self.get_recent_average('length', 100)
        
        print(f"\n{'='*60}")
        print(f"è®­ç»ƒç»Ÿè®¡ (å›žåˆ {self.episode_count})")
        print(f"{'='*60}")
        print(f"è¿è¡Œæ—¶é—´: {elapsed_time/3600:.2f} å°æ—¶")
        print(f"æ€»æ­¥æ•°: {self.step_count:,}")
        print(f"æ€»æ›´æ–°æ¬¡æ•°: {self.update_count}")
        print(f"æœ€è¿‘100å›žåˆå¹³å‡å¥–åŠ±: {avg_reward:.2f}" if avg_reward else "å¹³å‡å¥–åŠ±: N/A")
        print(f"æœ€è¿‘100å›žåˆå¹³å‡é•¿åº¦: {avg_length:.1f}" if avg_length else "å¹³å‡é•¿åº¦: N/A")
        print(f"æœ€ä½³å¥–åŠ±: {self.best_reward:.2f} (å›žåˆ {self.best_episode})")
        
        # æ˜¾ç¤ºæœ€è¿‘çš„æŸå¤±ä¿¡æ¯
        recent_losses = ['update_policy_loss', 'update_value_loss', 'update_total_loss']
        for loss_name in recent_losses:
            if loss_name in self.metrics and self.metrics[loss_name]:
                recent_loss = self.metrics[loss_name][-1]
                display_name = loss_name.replace('update_', '').replace('_', ' ').title()
                print(f"{display_name}: {recent_loss:.4f}")
        
        print(f"{'='*60}\n")
    
    def save_training_log(self):
        """
        ä¿å­˜è®­ç»ƒæ—¥å¿—åˆ°JSONæ–‡ä»¶
        """
        log_data = {
            'experiment_info': {
                'start_time': datetime.fromtimestamp(self.start_time).isoformat(),
                'episode_count': self.episode_count,
                'step_count': self.step_count,
                'update_count': self.update_count,
                'best_reward': self.best_reward,
                'best_episode': self.best_episode,
            },
            'episode_metrics': dict(self.episode_metrics),
            'training_metrics': dict(self.metrics),
            'config': {
                'num_envs': Config.NUM_ENVS,
                'learning_rate': Config.LEARNING_RATE,
                'ppo_epochs': Config.PPO_EPOCHS,
                'clip_epsilon': Config.CLIP_EPSILON,
                'frame_stack': Config.FRAME_STACK,
            }
        }
        
        # ä¿å­˜åˆ°JSONæ–‡ä»¶
        log_file = os.path.join(self.log_dir, 'training_log.json')
        with open(log_file, 'w') as f:
            json.dump(log_data, f, indent=2)
        
        print(f"è®­ç»ƒæ—¥å¿—å·²ä¿å­˜åˆ°: {log_file}")
    
    def close(self):
        """
        å…³é—­æ—¥å¿—è®°å½•å™¨ï¼Œæ¸…ç†èµ„æº
        """
        if self.tensorboard_writer:
            self.tensorboard_writer.close()
        
        # ä¿å­˜æœ€ç»ˆæ—¥å¿—
        self.save_training_log()
        
    def __del__(self):
        """
        æžæž„å‡½æ•°ï¼Œç¡®ä¿èµ„æºè¢«æ­£ç¡®æ¸…ç†
        """
        self.close()


class PerformanceMonitor:
    """
    æ€§èƒ½ç›‘æŽ§å™¨ - ç›‘æŽ§è®­ç»ƒè¿‡ç¨‹ä¸­çš„ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ
    """
    
    def __init__(self):
        """
        åˆå§‹åŒ–æ€§èƒ½ç›‘æŽ§å™¨
        """
        self.gpu_available = False
        
        # å°è¯•å¯¼å…¥GPUç›‘æŽ§å·¥å…·
        try:
            import torch
            if torch.cuda.is_available():
                self.gpu_available = True
                self.device_count = torch.cuda.device_count()
        except ImportError:
            pass
    
    def get_gpu_memory_usage(self):
        """
        èŽ·å–GPUå†…å­˜ä½¿ç”¨æƒ…å†µ
        
        Returns:
            dict: GPUå†…å­˜ä½¿ç”¨ä¿¡æ¯
        """
        if not self.gpu_available:
            return {}
        
        import torch
        gpu_info = {}
        
        for i in range(self.device_count):
            memory_allocated = torch.cuda.memory_allocated(i) / 1024**3  # GB
            memory_cached = torch.cuda.memory_reserved(i) / 1024**3     # GB
            
            gpu_info[f'gpu_{i}_memory_allocated'] = memory_allocated
            gpu_info[f'gpu_{i}_memory_cached'] = memory_cached
        
        return gpu_info
    
    def get_system_info(self):
        """
        èŽ·å–ç³»ç»Ÿä¿¡æ¯
        
        Returns:
            dict: ç³»ç»Ÿèµ„æºä½¿ç”¨ä¿¡æ¯
        """
        import psutil
        
        # CPUä½¿ç”¨çŽ‡
        cpu_percent = psutil.cpu_percent(interval=1)
        
        # å†…å­˜ä½¿ç”¨æƒ…å†µ
        memory = psutil.virtual_memory()
        memory_percent = memory.percent
        memory_used_gb = memory.used / 1024**3
        
        system_info = {
            'cpu_percent': cpu_percent,
            'memory_percent': memory_percent,
            'memory_used_gb': memory_used_gb,
        }
        
        # æ·»åŠ GPUä¿¡æ¯
        system_info.update(self.get_gpu_memory_usage())
        
        return system_info


class ProgressTracker:
    """
    è®­ç»ƒè¿›åº¦è·Ÿè¸ªå™¨ - è·Ÿè¸ªè®­ç»ƒç›®æ ‡å®Œæˆæƒ…å†µ
    """
    
    def __init__(self, target_reward=3000, patience=100):
        """
        åˆå§‹åŒ–è¿›åº¦è·Ÿè¸ªå™¨
        
        Args:
            target_reward (float): ç›®æ ‡å¥–åŠ±å€¼
            patience (int): æ—©åœè€å¿ƒå€¼ï¼ˆå¤šå°‘å›žåˆæ²¡æœ‰æ”¹è¿›å°±åœæ­¢ï¼‰
        """
        self.target_reward = target_reward
        self.patience = patience
        
        self.best_avg_reward = float('-inf')
        self.episodes_without_improvement = 0
        self.target_achieved = False
        
        self.reward_history = deque(maxlen=100)  # ä¿å­˜æœ€è¿‘100å›žåˆçš„å¥–åŠ±
    
    def update(self, episode_reward):
        """
        æ›´æ–°è¿›åº¦è·Ÿè¸ª
        
        Args:
            episode_reward (float): å½“å‰å›žåˆå¥–åŠ±
            
        Returns:
            dict: è·Ÿè¸ªä¿¡æ¯
        """
        self.reward_history.append(episode_reward)
        
        # è®¡ç®—æœ€è¿‘100å›žåˆçš„å¹³å‡å¥–åŠ±
        if len(self.reward_history) >= 10:  # è‡³å°‘10å›žåˆæ‰å¼€å§‹è®¡ç®—
            avg_reward = np.mean(self.reward_history)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ”¹è¿›
            if avg_reward > self.best_avg_reward:
                self.best_avg_reward = avg_reward
                self.episodes_without_improvement = 0
            else:
                self.episodes_without_improvement += 1
            
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡
            if avg_reward >= self.target_reward and not self.target_achieved:
                self.target_achieved = True
                print(f"\nðŸŽ‰ ç›®æ ‡è¾¾æˆï¼å¹³å‡å¥–åŠ± {avg_reward:.2f} è¶…è¿‡ç›®æ ‡ {self.target_reward}")
        
        return {
            'avg_reward': np.mean(self.reward_history) if self.reward_history else 0,
            'best_avg_reward': self.best_avg_reward,
            'episodes_without_improvement': self.episodes_without_improvement,
            'target_achieved': self.target_achieved,
            'should_stop': self.episodes_without_improvement >= self.patience
        }