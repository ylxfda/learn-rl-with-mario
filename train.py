"""
PPOé©¬é‡Œå¥¥è®­ç»ƒè„šæœ¬

è¿™ä¸ªè„šæœ¬å®ç°äº†å®Œæ•´çš„PPOè®­ç»ƒæµç¨‹ï¼š
1. åˆ›å»ºå¹¶è¡Œé©¬é‡Œå¥¥ç¯å¢ƒ
2. åˆå§‹åŒ–PPOç®—æ³•
3. æ•°æ®æ”¶é›†å’Œç»éªŒå›æ”¾
4. ç½‘ç»œæ›´æ–°
5. æ€§èƒ½ç›‘æ§å’Œæ¨¡å‹ä¿å­˜

ä½¿ç”¨æ–¹æ³•:
python train.py
"""

import os
import time
import argparse
import numpy as np
import torch
from tqdm import tqdm

# å¯¼å…¥æˆ‘ä»¬çš„æ¨¡å—
from config import Config
from enviroments.parallel_envs import create_parallel_mario_envs
from algorithms.ppo import create_ppo_algorithm
from utils.replay_buffer import RolloutBuffer
from utils.logger import TrainingLogger, PerformanceMonitor, ProgressTracker
from algorithms.base import ModelManager

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='PPO Mario Training')
    
    # ç¯å¢ƒå‚æ•°
    parser.add_argument('--num_envs', type=int, default=Config.NUM_ENVS,
                       help='å¹¶è¡Œç¯å¢ƒæ•°é‡')
    parser.add_argument('--worlds', nargs='+', default=['1-1', '1-2', '1-3', '1-4'],
                       help='è®­ç»ƒä½¿ç”¨çš„å…³å¡åˆ—è¡¨')
    parser.add_argument('--render_env', type=int, default=None,
                       help='éœ€è¦æ¸²æŸ“çš„ç¯å¢ƒIDï¼ˆç”¨äºè§‚å¯Ÿè®­ç»ƒè¿‡ç¨‹ï¼‰')
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--max_episodes', type=int, default=Config.MAX_EPISODES,
                       help='æœ€å¤§è®­ç»ƒå›åˆæ•°')
    parser.add_argument('--max_steps', type=int, default=Config.MAX_STEPS,
                       help='æœ€å¤§è®­ç»ƒæ­¥æ•°')
    parser.add_argument('--save_freq', type=int, default=Config.SAVE_FREQ,
                       help='ä¿å­˜æ¨¡å‹é¢‘ç‡ï¼ˆæŒ‰æ›´æ–°æ¬¡æ•°ï¼‰')
    parser.add_argument('--log_freq', type=int, default=Config.LOG_FREQ,
                       help='æ—¥å¿—è®°å½•é¢‘ç‡ï¼ˆæŒ‰æ›´æ–°æ¬¡æ•°ï¼‰')
    
    # PPOå‚æ•°
    parser.add_argument('--learning_rate', type=float, default=Config.LEARNING_RATE,
                       help='å­¦ä¹ ç‡')
    parser.add_argument('--ppo_epochs', type=int, default=Config.PPO_EPOCHS,
                       help='PPOæ›´æ–°è½®æ•°')
    parser.add_argument('--clip_epsilon', type=float, default=Config.CLIP_EPSILON,
                       help='PPOè£å‰ªå‚æ•°')
    parser.add_argument('--steps_per_update', type=int, default=Config.STEPS_PER_UPDATE,
                       help='æ¯æ¬¡æ›´æ–°æ”¶é›†çš„æ­¥æ•°')
    
    # ç³»ç»Ÿå‚æ•°
    parser.add_argument('--device', type=str, default=None,
                       help='è®¡ç®—è®¾å¤‡ (cuda/cpu)')
    parser.add_argument('--seed', type=int, default=Config.SEED,
                       help='éšæœºç§å­')
    parser.add_argument('--resume', type=str, default=None,
                       help='æ¢å¤è®­ç»ƒçš„æ¨¡å‹è·¯å¾„')
    parser.add_argument('--experiment_name', type=str, default=None,
                       help='å®éªŒåç§°')
    
    return parser.parse_args()


def set_seed(seed):
    """è®¾ç½®éšæœºç§å­"""
    torch.manual_seed(seed)
    np.random.seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    
    # è®¾ç½®ç¡®å®šæ€§è®¡ç®—ï¼ˆå¯èƒ½å½±å“æ€§èƒ½ï¼‰
    # torch.backends.cudnn.deterministic = True
    # torch.backends.cudnn.benchmark = False


class PPOTrainer:
    """PPOè®­ç»ƒå™¨ç±»"""
    
    def __init__(self, args):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            args: å‘½ä»¤è¡Œå‚æ•°
        """
        self.args = args
        self.device = torch.device(args.device) if args.device else Config.DEVICE
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # è®¾ç½®éšæœºç§å­
        set_seed(args.seed)
        
        # æ›´æ–°é…ç½®ï¼ˆæ”¯æŒå‘½ä»¤è¡Œå‚æ•°è¦†ç›–ï¼‰
        Config.LEARNING_RATE = args.learning_rate
        Config.PPO_EPOCHS = args.ppo_epochs
        Config.CLIP_EPSILON = args.clip_epsilon
        Config.STEPS_PER_UPDATE = args.steps_per_update
        
        # æ‰“å°é…ç½®ä¿¡æ¯
        Config.print_config()
        
        # åˆ›å»ºç›®å½•
        os.makedirs(Config.MODEL_DIR, exist_ok=True)
        os.makedirs(Config.LOG_DIR, exist_ok=True)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self._init_environment()
        self._init_algorithm() 
        self._init_buffer()
        self._init_logging()
        self._init_monitoring()
        
        print("PPOè®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆï¼")
    
    def _init_environment(self):
        """åˆå§‹åŒ–ç¯å¢ƒ"""
        print("åˆ›å»ºå¹¶è¡Œé©¬é‡Œå¥¥ç¯å¢ƒ...")
        
        self.envs = create_parallel_mario_envs(
            num_envs=self.args.num_envs,
            worlds=self.args.worlds,
            use_subprocess=True,  # ä½¿ç”¨å¤šè¿›ç¨‹ä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½
            render_env_id=self.args.render_env
        )
        
        self.observation_space = self.envs.observation_space
        self.action_space = self.envs.action_space
        
        print(f"ç¯å¢ƒåˆ›å»ºå®Œæˆ: {len(self.envs)} ä¸ªå¹¶è¡Œç¯å¢ƒ")
    
    def _init_algorithm(self):
        """åˆå§‹åŒ–PPOç®—æ³•"""
        print("åˆå§‹åŒ–PPOç®—æ³•...")
        
        self.ppo = create_ppo_algorithm(
            observation_space=self.observation_space,
            action_space=self.action_space,
            device=self.device,
            logger=None  # ç¨åè®¾ç½®
        )
        
        # å¦‚æœæœ‰æ¢å¤çš„æ¨¡å‹ï¼ŒåŠ è½½å®ƒ
        if self.args.resume:
            print(f"ä» {self.args.resume} æ¢å¤è®­ç»ƒ...")
            model_manager = ModelManager()
            model_manager.load_model(self.ppo, self.args.resume)
    
    def _init_buffer(self):
        """åˆå§‹åŒ–ç»éªŒç¼“å†²åŒº"""
        print("åˆå§‹åŒ–ç»éªŒç¼“å†²åŒº...")
        
        self.rollout_buffer = RolloutBuffer(
            buffer_size=Config.STEPS_PER_UPDATE,
            num_envs=self.args.num_envs,
            obs_shape=self.observation_space.shape,
            action_dim=1,  # ç¦»æ•£åŠ¨ä½œ
            device=self.device
        )
        
        print(f"ç¼“å†²åŒºå¤§å°: {len(self.rollout_buffer):,} ä¸ªè½¬ç§»")
    
    def _init_logging(self):
        """åˆå§‹åŒ–æ—¥å¿—è®°å½•"""
        print("åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ...")
        
        self.logger = TrainingLogger(
            log_dir=Config.LOG_DIR,
            experiment_name=self.args.experiment_name
        )
        
        # è®¾ç½®ç®—æ³•çš„æ—¥å¿—è®°å½•å™¨
        self.ppo.logger = self.logger
        
        # è¿›åº¦è·Ÿè¸ªå™¨
        self.progress_tracker = ProgressTracker(
            target_reward=Config.TARGET_REWARD,
            patience=Config.PATIENCE  # åˆæ²¡æœ‰æ”¹è¿›å°±å¯ä»¥è€ƒè™‘åœæ­¢
        )
    
    def _init_monitoring(self):
        """åˆå§‹åŒ–æ€§èƒ½ç›‘æ§"""
        self.performance_monitor = PerformanceMonitor()
        self.model_manager = ModelManager()
        
        # è®­ç»ƒç»Ÿè®¡
        self.episode_rewards = []
        self.episode_lengths = []
        self.best_avg_reward = float('-inf')
        self.episodes_since_best = 0
    
    def collect_rollouts(self):
        """
        æ”¶é›†ä¸€æ‰¹è®­ç»ƒæ•°æ®
        
        Returns:
            dict: æ”¶é›†ç»Ÿè®¡ä¿¡æ¯
        """
        self.ppo.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼ˆå…³é—­dropoutç­‰ï¼‰
        
        # é‡ç½®ç¼“å†²åŒº
        self.rollout_buffer.reset()
        
        # é‡ç½®ç¯å¢ƒå¹¶è·å–åˆå§‹çŠ¶æ€
        observations = self.envs.reset()
        
        # æ”¶é›†ç»Ÿè®¡
        collect_stats = {
            'episodes_completed': 0,
            'total_reward': 0.0,
            'avg_episode_length': 0.0,
        }
        
        episode_rewards = []
        episode_lengths = []
        current_episode_rewards = np.zeros(self.args.num_envs)
        current_episode_lengths = np.zeros(self.args.num_envs)
        
        # æ”¶é›†æŒ‡å®šæ­¥æ•°çš„æ•°æ®
        for step in range(Config.STEPS_PER_UPDATE):
            # é€‰æ‹©åŠ¨ä½œ
            with torch.no_grad():
                actions, extra_info = self.ppo.act(observations)
                values = extra_info['values']
                log_probs = extra_info['log_probs']
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_observations, rewards, dones, infos = self.envs.step(actions)
            
            # å­˜å‚¨è½¬ç§»
            self.rollout_buffer.add(
                states=observations,
                actions=actions,
                rewards=rewards,
                values=values,
                log_probs=log_probs,
                dones=dones
            )
            
            # æ›´æ–°ç»Ÿè®¡
            current_episode_rewards += rewards.cpu().numpy()
            current_episode_lengths += 1
            
            # å¤„ç†å›åˆç»“æŸ
            for i, done in enumerate(dones):
                if done:
                    episode_reward = current_episode_rewards[i]
                    episode_length = current_episode_lengths[i]
                    
                    episode_rewards.append(episode_reward)
                    episode_lengths.append(episode_length)
                    
                    # è®°å½•åˆ°æ—¥å¿—
                    info = infos[i] if i < len(infos) else {}
                    self.logger.log_episode(episode_reward, episode_length, info)
                    
                    # æ›´æ–°è¿›åº¦è·Ÿè¸ª
                    progress_info = self.progress_tracker.update(episode_reward)
                    
                    # é‡ç½®è®¡æ•°å™¨
                    current_episode_rewards[i] = 0
                    current_episode_lengths[i] = 0
                    
                    collect_stats['episodes_completed'] += 1
            
            # æ›´æ–°è§‚å¯Ÿ
            observations = next_observations
        
        # è®¡ç®—æœ€åçŠ¶æ€çš„ä»·å€¼ï¼ˆç”¨äºGAEè®¡ç®—ï¼‰
        with torch.no_grad():
            next_values = self.ppo.compute_value(next_observations)
        
        # è®¡ç®—ä¼˜åŠ¿å’Œå›æŠ¥
        self.rollout_buffer.compute_advantages_and_returns(
            next_values=next_values,
            gamma=Config.GAMMA,
            gae_lambda=Config.GAE_LAMBDA
        )
        
        # æ›´æ–°æ”¶é›†ç»Ÿè®¡
        if episode_rewards:
            collect_stats['total_reward'] = sum(episode_rewards)
            collect_stats['avg_episode_length'] = np.mean(episode_lengths)
            
            # æ›´æ–°å…¨å±€ç»Ÿè®¡
            self.episode_rewards.extend(episode_rewards)
            self.episode_lengths.extend(episode_lengths)
        
        return collect_stats
    
    def train_step(self):
        """
        æ‰§è¡Œä¸€æ¬¡å®Œæ•´çš„è®­ç»ƒæ­¥éª¤
        
        Returns:
            dict: è®­ç»ƒç»Ÿè®¡ä¿¡æ¯
        """
        # 1. æ”¶é›†æ•°æ®
        collect_stats = self.collect_rollouts()
        
        # 2. æ›´æ–°ç­–ç•¥
        self.ppo.train()  # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
        update_stats = self.ppo.update(self.rollout_buffer)
        
        # 3. åˆå¹¶ç»Ÿè®¡ä¿¡æ¯
        train_stats = {**collect_stats, **update_stats}
        
        # 4. æ›´æ–°æ€»æ­¥æ•°
        self.ppo.total_steps += Config.STEPS_PER_UPDATE * self.args.num_envs
        
        return train_stats
    
    def evaluate_model(self, num_episodes=5):
        """
        è¯„ä¼°å½“å‰æ¨¡å‹æ€§èƒ½
        
        Args:
            num_episodes (int): è¯„ä¼°å›åˆæ•°
            
        Returns:
            dict: è¯„ä¼°ç»“æœ
        """
        print(f"è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼ˆ{num_episodes} å›åˆï¼‰...")
        
        self.ppo.eval()
        
        # åˆ›å»ºå•ä¸ªç¯å¢ƒç”¨äºè¯„ä¼°ï¼ˆä¸æ¸²æŸ“ï¼‰
        from enviroments.mario_env import create_mario_environment
        eval_env = create_mario_environment('1-1')
        
        eval_rewards = []
        eval_lengths = []
        
        for episode in range(num_episodes):
            obs = eval_env.reset()
            obs = torch.FloatTensor(obs).unsqueeze(0).to(self.device)
            
            episode_reward = 0
            episode_length = 0
            done = False
            
            while not done:
                with torch.no_grad():
                    actions, _ = self.ppo.act(obs, deterministic=True)  # ç¡®å®šæ€§åŠ¨ä½œ
                
                obs, reward, done, info = eval_env.step(actions.item())
                obs = torch.FloatTensor(obs).unsqueeze(0).to(self.device)
                
                episode_reward += reward
                episode_length += 1
                
                # é˜²æ­¢æ— é™å¾ªç¯
                if episode_length > 5000:
                    break
            
            eval_rewards.append(episode_reward)
            eval_lengths.append(episode_length)
            
            print(f"  è¯„ä¼°å›åˆ {episode+1}: å¥–åŠ±={episode_reward:.2f}, é•¿åº¦={episode_length}")
        
        eval_env.close()
        
        eval_stats = {
            'eval_avg_reward': np.mean(eval_rewards),
            'eval_std_reward': np.std(eval_rewards),
            'eval_max_reward': np.max(eval_rewards),
            'eval_min_reward': np.min(eval_rewards),
            'eval_avg_length': np.mean(eval_lengths),
        }
        
        print(f"è¯„ä¼°å®Œæˆ: å¹³å‡å¥–åŠ±={eval_stats['eval_avg_reward']:.2f} Â± {eval_stats['eval_std_reward']:.2f}")
        
        return eval_stats
    
    def should_stop_training(self):
        """
        åˆ¤æ–­æ˜¯å¦åº”è¯¥åœæ­¢è®­ç»ƒ
        
        Returns:
            tuple: (æ˜¯å¦åœæ­¢, åœæ­¢åŸå› )
        """
        # æ£€æŸ¥æœ€å¤§æ­¥æ•°
        if self.ppo.total_steps >= self.args.max_steps:
            return True, f"è¾¾åˆ°æœ€å¤§æ­¥æ•° {self.args.max_steps:,}"
        
        # æ£€æŸ¥æœ€å¤§å›åˆæ•°
        if self.ppo.total_episodes >= self.args.max_episodes:
            return True, f"è¾¾åˆ°æœ€å¤§å›åˆæ•° {self.args.max_episodes:,}"
        
        # æ£€æŸ¥ç›®æ ‡å¥–åŠ±
        if len(self.episode_rewards) >= 100:
            recent_avg = np.mean(self.episode_rewards[-100:])
            if recent_avg >= Config.TARGET_REWARD:
                return True, f"è¾¾åˆ°ç›®æ ‡å¥–åŠ± {Config.TARGET_REWARD} (å½“å‰: {recent_avg:.2f})"
        
        # æ£€æŸ¥æ—©åœæ¡ä»¶
        progress_info = self.progress_tracker.update(
            self.episode_rewards[-1] if self.episode_rewards else 0
        )
        if progress_info['should_stop']:
            return True, f"æ—©åœï¼šè¿ç»­ {progress_info['episodes_without_improvement']} å›åˆæ— æ”¹è¿›"
        
        return False, ""
    
    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        print("\nå¼€å§‹PPOè®­ç»ƒ...")
        print("=" * 60)
        
        start_time = time.time()
        update_count = 0
        
        try:
            while True:
                update_start_time = time.time()
                
                # æ‰§è¡Œè®­ç»ƒæ­¥éª¤
                train_stats = self.train_step()
                update_count += 1
                
                # è®°å½•ç³»ç»Ÿä¿¡æ¯
                if self.performance_monitor:
                    system_info = self.performance_monitor.get_system_info()
                    self.logger.log_system_info(**system_info)
                
                # å®šæœŸæ‰“å°ç»Ÿè®¡ä¿¡æ¯
                if update_count % self.args.log_freq == 0:
                    update_time = time.time() - update_start_time
                    total_time = time.time() - start_time
                    
                    print(f"\næ›´æ–° #{update_count}")
                    print(f"æ€»æ­¥æ•°: {self.ppo.total_steps:,}")
                    print(f"æ€»å›åˆ: {self.ppo.total_episodes:,}")
                    print(f"æ›´æ–°ç”¨æ—¶: {update_time:.2f}s")
                    print(f"æ€»ç”¨æ—¶: {total_time/3600:.2f}h")
                    
                    if train_stats.get('episodes_completed', 0) > 0:
                        print(f"å®Œæˆå›åˆ: {train_stats['episodes_completed']}")
                        print(f"å¹³å‡å¥–åŠ±: {train_stats['total_reward']/train_stats['episodes_completed']:.2f}")
                    
                    print(f"ç­–ç•¥æŸå¤±: {train_stats.get('policy_loss', 0):.4f}")
                    print(f"ä»·å€¼æŸå¤±: {train_stats.get('value_loss', 0):.4f}")
                    print(f"ç†µ: {train_stats.get('entropy', 0):.4f}")
                    print(f"è£å‰ªæ¯”ä¾‹: {train_stats.get('clip_fraction', 0):.3f}")
                    print(f"å­¦ä¹ ç‡: {train_stats.get('learning_rate', 0):.2e}")
                    
                    # æ˜¾ç¤ºæœ€è¿‘è¡¨ç°
                    if len(self.episode_rewards) >= 10:
                        recent_10 = np.mean(self.episode_rewards[-10:])
                        recent_100 = np.mean(self.episode_rewards[-100:]) if len(self.episode_rewards) >= 100 else np.mean(self.episode_rewards)
                        print(f"æœ€è¿‘10å›åˆå¹³å‡å¥–åŠ±: {recent_10:.2f}")
                        print(f"æœ€è¿‘100å›åˆå¹³å‡å¥–åŠ±: {recent_100:.2f}")
                        print(f"å†å²æœ€ä½³å¥–åŠ±: {max(self.episode_rewards):.2f}")
                
                # å®šæœŸä¿å­˜æ¨¡å‹
                if update_count % self.args.save_freq == 0:
                    # è¯„ä¼°æ¨¡å‹
                    eval_stats = self.evaluate_model(num_episodes=3)
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹
                    current_avg = eval_stats['eval_avg_reward']
                    is_best = current_avg > self.best_avg_reward
                    
                    if is_best:
                        self.best_avg_reward = current_avg
                        self.episodes_since_best = 0
                        print(f"ğŸ‰ å‘ç°æ›´å¥½çš„æ¨¡å‹! å¹³å‡å¥–åŠ±: {current_avg:.2f}")
                    else:
                        self.episodes_since_best += 1
                    
                    # ä¿å­˜æ¨¡å‹
                    model_filename = f"ppo_mario_update_{update_count}.pth"
                    self.model_manager.save_model(
                        self.ppo, 
                        filename=model_filename,
                        is_best=is_best
                    )
                    
                    # è®°å½•è¯„ä¼°ç»“æœ
                    self.logger.log_training_step(**eval_stats)
                
                # æ£€æŸ¥åœæ­¢æ¡ä»¶
                should_stop, stop_reason = self.should_stop_training()
                if should_stop:
                    print(f"\nè®­ç»ƒåœæ­¢: {stop_reason}")
                    break
                
                # æ¯100æ¬¡æ›´æ–°æ˜¾ç¤ºè¯¦ç»†ç»Ÿè®¡
                if update_count % 100 == 0:
                    self.logger.print_training_stats()
                    
                    # æ˜¾ç¤ºç¯å¢ƒç»Ÿè®¡
                    env_stats = self.envs.get_statistics()
                    print("ç¯å¢ƒç»Ÿè®¡:")
                    for key, value in env_stats.items():
                        if isinstance(value, float):
                            print(f"  {key}: {value:.4f}")
                        else:
                            print(f"  {key}: {value}")
        
        except KeyboardInterrupt:
            print("\næ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œä¿å­˜æ¨¡å‹å¹¶é€€å‡º...")
            
        except Exception as e:
            print(f"\nè®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
        
        finally:
            # æœ€ç»ˆä¿å­˜
            final_model_path = "ppo_mario_final.pth"
            self.model_manager.save_model(self.ppo, filename=final_model_path)
            
            # æœ€ç»ˆè¯„ä¼°
            final_eval = self.evaluate_model(num_episodes=10)
            print(f"\næœ€ç»ˆè¯„ä¼°ç»“æœ:")
            for key, value in final_eval.items():
                print(f"  {key}: {value:.4f}")
            
            # æ¸…ç†èµ„æº
            self.envs.close()
            self.logger.close()
            
            total_time = time.time() - start_time
            print(f"\nè®­ç»ƒå®Œæˆ! æ€»ç”¨æ—¶: {total_time/3600:.2f} å°æ—¶")
            print(f"æœ€ç»ˆæ¨¡å‹ä¿å­˜åœ¨: {final_model_path}")


def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_args()
    
    print("PPOé©¬é‡Œå¥¥è®­ç»ƒ")
    print("=" * 60)
    print(f"è®¾å¤‡: {torch.device(args.device) if args.device else Config.DEVICE}")
    print(f"å¹¶è¡Œç¯å¢ƒæ•°: {args.num_envs}")
    print(f"è®­ç»ƒå…³å¡: {args.worlds}")
    print(f"æœ€å¤§å›åˆæ•°: {args.max_episodes:,}")
    print(f"æœ€å¤§æ­¥æ•°: {args.max_steps:,}")
    print("=" * 60)
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = PPOTrainer(args)
    
    # å¼€å§‹è®­ç»ƒ
    trainer.train()


if __name__ == "__main__":
    main()